{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 558 Homework 3\n",
    "#### Anushna Prakash\n",
    "#### April 25, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "F(\\beta) = \\frac{1}{n} \\sum_{i=1}^n log(1 + e^{-y_i x_i^T \\beta}) + \\lambda ||\\beta||_2^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the case where $n=1$ and $d=1$. Then:\n",
    "$$\n",
    "F(\\beta) = log(1 + e^{-y_{i} x_{i}^T \\beta}) + \\lambda \\beta^2\n",
    "\\\\ \\nabla F(\\beta) = \\frac{-yx e^{-yx\\beta}}{1 + e^{-yx \\beta}} + 2\\lambda \\beta\n",
    "$$\n",
    "\n",
    "Now assuming the case where $n>1$ and $d>1$, and $y$ is $1xn$, $x$ is $dxn$ and $\\beta$ is $dx1$:\n",
    "$$\n",
    "\\nabla F(\\beta) = \\frac{1}{n} \\frac{-y x^T e^{-y x^T \\beta}}{1 + e^{-y x^T \\beta}} + 2 \\lambda \\beta \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "1  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "2  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "3  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "4  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "5  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Smarket.csv'\n",
    "smarket = pd.read_csv(file, sep = ',', header = 0, index_col = 0)\n",
    "smarket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 3)\n",
      "(1250,)\n"
     ]
    }
   ],
   "source": [
    "X = smarket[['Lag1', 'Lag2', 'Volume']].to_numpy()\n",
    "# y = {1, -1} for {Up, Down}\n",
    "y = smarket['Direction'].apply(lambda x: 1 if (x == 'Up') else -1).to_numpy()\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X, y, test_size = 0.2, random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize train and test sets on training data\n",
    "X_ss = StandardScaler().fit(X_train_us)\n",
    "X_train = X_ss.transform(X_train_us)\n",
    "X_test = X_ss.transform(X_test_us)\n",
    "\n",
    "y_mean = y_train_us.mean()\n",
    "y_sd = y_train_us.std()\n",
    "y_train = (y_train_us - y_mean) / y_sd\n",
    "y_test = (y_test_us - y_mean) / y_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(beta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the gradient for regularized logistic regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    X : ndarray\n",
    "        A numpy array of standardized feature variables with shape n x d\n",
    "    y : 1D array\n",
    "        A numpy array of standardized response variables with shape 1 x n\n",
    "    lambda_ : float\n",
    "        Regularization parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : 1D array\n",
    "        A numpy array of the gradient with the given X, y, and lambda with shape 1 x d\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    x_tb = np.dot(X, beta)\n",
    "    e_ = np.exp(np.dot(-y, x_tb))\n",
    "    # Debugging print statements\n",
    "#     print(\"n: \", n)\n",
    "#     print(\"x_tb: \", x_tb.shape)\n",
    "#     print(\"e_: \", e_.shape)\n",
    "#     print(\"numerator: \", np.dot(np.dot(-y.T, X), e_).shape)\n",
    "#     print(\"denominator: \", (e_ + 1))\n",
    "    return (1/n) * np.dot(np.dot(-y.T, X), e_) / (e_ + 1) + 2 * lambda_ * beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeobj(beta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the gradient for regularized logistic regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    X : ndarray\n",
    "        A numpy array of standardized feature variables with shape n x d\n",
    "    y : 1D array\n",
    "        A numpy array of standardized response variables with shape 1 x n\n",
    "    lambda_ : float\n",
    "        Regularization parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    obj : float\n",
    "        Value of the objective function evaluated with the given parameters\n",
    "    \"\"\"\n",
    "    x_tb = np.dot(X, beta)\n",
    "    e_ = np.exp(np.dot(-y, x_tb))\n",
    "    return np.mean(np.log(1 + e_)) + lambda_ * np.linalg.norm(beta)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(eta_init, decay_rate, prop, p, beta, *args):\n",
    "    \"\"\"\n",
    "    Returns the optimal eta for fast gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta_init : float\n",
    "    decay_rate : float\n",
    "        The rate at which eta will decay for search\n",
    "    prop : float\n",
    "        Constant for proportionality between descent and directional derivative\n",
    "    p : 1D array\n",
    "        A numpy array with shape 1 x d representing the direction\n",
    "    beta : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    *args : X, y, lambda_\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eta : float\n",
    "        Optimal size of eta for fast gradient descent\n",
    "    \"\"\"\n",
    "    eta = eta_init\n",
    "    beta = beta\n",
    "    \n",
    "    def decrease_condition(eta):\n",
    "        \"\"\"\n",
    "        Compute if the difference in objective functions evaluated with new vs. old beta\n",
    "        is less than ???\n",
    "        \"\"\"\n",
    "        left = computeobj(beta + eta * p, *args) - computeobj(beta, *args)\n",
    "        right = eta * prop * np.dot(computegrad(beta, *args), p)\n",
    "        return left <= right\n",
    "    \n",
    "    while not decrease_condition(eta):\n",
    "        eta *= decay_rate\n",
    "    \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddescent(tolerance, eta_init, decay_rate, prop, p, beta_init, *args):\n",
    "    \"\"\"\n",
    "    Returns the optimal eta for fast gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tolerance: float\n",
    "        A total error tolerance\n",
    "    eta_init : float\n",
    "    decay_rate : float\n",
    "        The rate at which eta will decay for search\n",
    "    prop : float\n",
    "        Constant for proportionality between descent and directional derivative\n",
    "    p : 1D array\n",
    "        A numpy array with shape 1 x d representing the initial starting point\n",
    "    beta_init : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    *args : X, y, lambda_\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iterations : float\n",
    "        Optimal size of eta for fast gradient descent\n",
    "    \"\"\"\n",
    "    iterations = [beta_init]\n",
    "    grad = computegrad(beta_init, *args)\n",
    "    beta = beta_init\n",
    "    \n",
    "    while np.linalg.norm(grad) > tolerance:\n",
    "        eta = backtracking(eta_init, decay_rate, prop, p, beta, *args)\n",
    "        beta = beta - eta * computegrad(beta, *args)\n",
    "        grad = computegrad(beta, *args)\n",
    "        iterations.append[beta]\n",
    "    \n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastgradalgo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
