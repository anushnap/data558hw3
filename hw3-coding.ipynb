{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 558 Homework 3\n",
    "#### Anushna Prakash\n",
    "#### April 25, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "F(\\beta) = \\frac{1}{n} \\sum_{i=1}^n log(1 + e^{-y_i x_i^T \\beta}) + \\lambda ||\\beta||_2^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the case where $n=1$ and $d=1$. Then:\n",
    "$$\n",
    "F(\\beta) = log(1 + e^{-y_{i} x_{i}^T \\beta}) + \\lambda \\beta^2\n",
    "\\\\ \\nabla F(\\beta) = \\frac{-yx e^{-yx\\beta}}{1 + e^{-yx \\beta}} + 2\\lambda \\beta\n",
    "$$\n",
    "\n",
    "Now assuming the case where $n>1$ and $d>1$, and $y$ is $1xn$, $x$ is $dxn$ and $\\beta$ is $dx1$:\n",
    "$$\n",
    "\\nabla F(\\beta) = \\frac{1}{n} \\frac{-y x^T e^{-y x^T \\beta}}{1 + e^{-y x^T \\beta}} + 2 \\lambda \\beta \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "1  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "2  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "3  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "4  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "5  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Smarket.csv'\n",
    "smarket = pd.read_csv(file, sep = ',', header = 0, index_col = 0)\n",
    "smarket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 3)\n",
      "(1250,)\n"
     ]
    }
   ],
   "source": [
    "X = smarket[['Lag1', 'Lag2', 'Volume']].to_numpy()\n",
    "# y = {1, -1} for {Up, Down}\n",
    "y = smarket['Direction'].apply(lambda x: 1 if (x == 'Up') else -1).to_numpy()\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train_us, X_test_us, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize train and test sets on training data\n",
    "X_ss = StandardScaler().fit(X_train_us)\n",
    "X_train = X_ss.transform(X_train_us)\n",
    "X_test = X_ss.transform(X_test_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(beta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the gradient for regularized logistic regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    X : ndarray\n",
    "        A numpy array of standardized feature variables with shape n x d\n",
    "    y : 1D array\n",
    "        A numpy array of standardized response variables with shape 1 x n\n",
    "    lambda_ : float\n",
    "        Regularization parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : 1D array\n",
    "        A numpy array of the gradient with the given X, y, and lambda with shape 1 x d\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    x_tb = np.dot(X, beta)\n",
    "    e_ = np.exp(np.dot(-y, x_tb))\n",
    "    # Debugging print statements\n",
    "#     print(\"n: \", n)\n",
    "#     print(\"x_tb: \", x_tb.shape)\n",
    "#     print(\"e_: \", e_.shape)\n",
    "#     print(\"numerator: \", np.dot(np.dot(-y.T, X), e_).shape)\n",
    "#     print(\"denominator: \", (e_ + 1))\n",
    "    return (1/n) * np.dot(np.dot(-y.T, X), e_) / (e_ + 1) + 2 * lambda_ * beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeobj(beta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the gradient for regularized logistic regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    X : ndarray\n",
    "        A numpy array of standardized feature variables with shape n x d\n",
    "    y : 1D array\n",
    "        A numpy array of standardized response variables with shape 1 x n\n",
    "    lambda_ : float\n",
    "        Regularization parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    obj : float\n",
    "        Value of the objective function evaluated with the given parameters\n",
    "    \"\"\"\n",
    "    x_tb = np.dot(X, beta)\n",
    "    e_ = np.exp(np.dot(-y, x_tb))\n",
    "    return np.mean(np.log(1 + e_)) + lambda_ * np.linalg.norm(beta)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(eta_init, decay_rate, prop, p, beta, *args):\n",
    "    \"\"\"\n",
    "    Returns the optimal eta for fast gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta_init : float\n",
    "    decay_rate : float\n",
    "        The rate at which eta will decay for search\n",
    "    prop : float\n",
    "        Constant for proportionality between descent and directional derivative\n",
    "    p : 1D array\n",
    "        A numpy array with shape 1 x d representing the direction\n",
    "    beta : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    *args : X, y, lambda_\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eta : float\n",
    "        Optimal size of eta for fast gradient descent\n",
    "    \"\"\"\n",
    "    eta = eta_init\n",
    "    \n",
    "    def decrease_condition(eta):\n",
    "        \"\"\"\n",
    "        Compute if the difference in objective functions evaluated with new vs. old beta\n",
    "        is less than ???\n",
    "        \"\"\"\n",
    "        left = computeobj(beta + eta * p, *args) - computeobj(beta, *args)\n",
    "        right = eta * prop * np.dot(computegrad(beta, *args), p)\n",
    "        return left <= right\n",
    "    \n",
    "    while not decrease_condition(eta):\n",
    "        eta *= decay_rate\n",
    "    \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddescent(tolerance, eta_init, decay_rate, prop, beta_init, *args):\n",
    "    \"\"\"\n",
    "    Returns the optimal eta for fast gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tolerance: float\n",
    "        A total error tolerance\n",
    "    eta_init : float\n",
    "    decay_rate : float\n",
    "        The rate at which eta will decay for search\n",
    "    prop : float\n",
    "        Constant for proportionality between descent and directional derivative\n",
    "    beta_init : 1D array\n",
    "        A numpy array with shape 1 x d\n",
    "    *args : X, y, lambda_\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iterations : list\n",
    "        List of betas with most optimal beta being at [-1] index\n",
    "    \"\"\"\n",
    "    iterations = [beta_init]\n",
    "    grad = computegrad(beta_init, *args)\n",
    "    p = -grad\n",
    "    beta = beta_init\n",
    "    \n",
    "    while np.linalg.norm(grad) > tolerance:\n",
    "        eta = backtracking(eta_init, decay_rate, prop, p, beta, *args)\n",
    "        beta = beta - eta * computegrad(beta, *args)\n",
    "        grad = computegrad(beta, *args)\n",
    "        p = -grad\n",
    "        iterations.append(beta)\n",
    "    \n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastgradalgo(tolerance, eta_init, decay_rate, prop, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the optimal eta for fast gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tolerance: float\n",
    "        A total error tolerance\n",
    "    eta_init : float\n",
    "    decay_rate : float\n",
    "        The rate at which eta will decay for search\n",
    "    prop : float\n",
    "        Constant for proportionality between descent and directional derivative\n",
    "    num_features : int\n",
    "        Number of features in data set\n",
    "    *args : X, y, lambda_\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iterations : list\n",
    "        List of betas with most optimal beta being at [-1] index\n",
    "    \"\"\"\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    iterations = [beta]\n",
    "    grad = computegrad(beta, X, y, lambda_)\n",
    "    p = -grad\n",
    "    t = 0\n",
    "    \n",
    "    while np.linalg.norm(grad) > tolerance:\n",
    "        eta = backtracking(eta_init, decay_rate, prop, p, beta, X, y, lambda_)\n",
    "        beta_t = theta - eta * computegrad(theta, X, y, lambda_)\n",
    "        theta = beta_t + (t / (t + 3)) * (beta_t - beta)\n",
    "        grad = computegrad(beta_t, X, y, lambda_)\n",
    "        p = -grad\n",
    "        beta = beta_t\n",
    "        t += 1\n",
    "        iterations.append(beta)\n",
    "    \n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "eta_init = 0.1\n",
    "error = 5e-4\n",
    "lambda_ = 0.5\n",
    "decay_rate = 0.8\n",
    "prop = 0.3\n",
    "beta_init = np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_descent = graddescent(error, eta_init, decay_rate, prop, beta_init, X_train, y_train, lambda_)\n",
    "f_slow = [computeobj(b, X_train, y_train, lambda_) for b in grad_descent]\n",
    "fast_grad_descent = fastgradalgo(error, eta_init, decay_rate, prop, X_train, y_train, lambda_)\n",
    "f_fast = [computeobj(b, X_train, y_train, lambda_) for b in fast_grad_descent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd7144a31f0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f_slow, c = 'cornflowerblue')\n",
    "plt.plot(f_fast, c = 'chocolate')\n",
    "plt.title('Objective Function vs. iterations')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('F(B)')\n",
    "plt.legend(['Grad desc', 'Fast grad desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4974582  0.50726414 0.49887796 0.50175681 0.49520942 0.49709846\n",
      " 0.49414106 0.50053686 0.49820753 0.49835742 0.49980638 0.50223639\n",
      " 0.49863828 0.49369059 0.49672562 0.50065198 0.50080851 0.5080181\n",
      " 0.49949463 0.49600675 0.50283245 0.495799   0.50196772 0.4990431\n",
      " 0.50274701 0.50043015 0.49362732 0.49516477 0.50053667 0.49979318\n",
      " 0.49614528 0.49669266 0.4972364  0.50125726 0.50978758 0.4965573\n",
      " 0.50457899 0.49865858 0.49865924 0.49828528 0.48957757 0.49841112\n",
      " 0.49841523 0.50290597 0.49965349 0.49957888 0.49377328 0.49951006\n",
      " 0.49654851 0.49720066 0.49534296 0.50056842 0.49848521 0.49551147\n",
      " 0.49813219 0.50124139 0.51066781 0.50000121 0.50232067 0.50075799\n",
      " 0.50304657 0.4989669  0.4985234  0.501174   0.50299877 0.50522283\n",
      " 0.49955309 0.50425434 0.49727136 0.49891932 0.49416327 0.49957208\n",
      " 0.49956096 0.49096935 0.49586397 0.5018464  0.50464457 0.50271006\n",
      " 0.50022838 0.49966624 0.50048232 0.49694913 0.49755817 0.49945036\n",
      " 0.49928679 0.49979633 0.50954572 0.49780269 0.50232383 0.49730446\n",
      " 0.50087054 0.5026053  0.49643758 0.50508028 0.50837649 0.50090587\n",
      " 0.49389981 0.50413605 0.50186352 0.50554985 0.4957462  0.49830232\n",
      " 0.50464832 0.49666427 0.50199453 0.49947144 0.50091254 0.49720539\n",
      " 0.49479008 0.50499074 0.49972052 0.49961332 0.50389583 0.49851333\n",
      " 0.49395178 0.49730441 0.49159814 0.49950813 0.50316514 0.49910551\n",
      " 0.51074073 0.49717624 0.49809842 0.50384412 0.4959068  0.49833448\n",
      " 0.49810583 0.50005071 0.50393914 0.50807631 0.49760183 0.50274359\n",
      " 0.49874847 0.49709819 0.50170049 0.49315074 0.49917256 0.50698895\n",
      " 0.49724335 0.4966475  0.50681761 0.49845263 0.50267586 0.49455434\n",
      " 0.50228216 0.49876445 0.50071043 0.5022528  0.49397887 0.49711426\n",
      " 0.50392232 0.50088684 0.50159487 0.5041841  0.49607889 0.50245483\n",
      " 0.49655349 0.5093828  0.49685626 0.50295292 0.50018557 0.49611967\n",
      " 0.49856512 0.49666733 0.50366038 0.5066066  0.50529765 0.4968126\n",
      " 0.50340356 0.49589783 0.50132241 0.49478036 0.50043171 0.50504772\n",
      " 0.50085055 0.50190083 0.5001748  0.50106216 0.50563629 0.49925174\n",
      " 0.50124367 0.50405332 0.5090357  0.4974399  0.4985995  0.49709299\n",
      " 0.4997274  0.50576809 0.49742776 0.50885059 0.50981723 0.49919772\n",
      " 0.50348009 0.49845719 0.49719485 0.49773111 0.49979468 0.49842979\n",
      " 0.5002062  0.49773423 0.50008399 0.50193217 0.49843008 0.50242019\n",
      " 0.50134811 0.49609878 0.49819457 0.50496621 0.50224398 0.49745117\n",
      " 0.49674512 0.49719521 0.50072907 0.50227431 0.49918202 0.49335753\n",
      " 0.50267117 0.50951835 0.49906981 0.49863927 0.50381034 0.49747354\n",
      " 0.50203587 0.49953586 0.50364283 0.49842043 0.50574084 0.50200341\n",
      " 0.50459875 0.5012113  0.50404645 0.50576884 0.5009023  0.49938958\n",
      " 0.49928506 0.49857295 0.50353685 0.51847616 0.49626131 0.50036205\n",
      " 0.50557883 0.49702813 0.50614784 0.50154039 0.50275291 0.49835889\n",
      " 0.50253526 0.49727801 0.50216904 0.49340367 0.50508129 0.49752794\n",
      " 0.50228946 0.50142617 0.49754196 0.50101145 0.49675748 0.49904444\n",
      " 0.50441779 0.50261176 0.50427298 0.49743927 0.49858034 0.49768384\n",
      " 0.49923778 0.49862364 0.49925101 0.49201667 0.50337752 0.49710534\n",
      " 0.5032619  0.50070684 0.51364984 0.49552166 0.49991174 0.49828146\n",
      " 0.49612534 0.50856807 0.49882158 0.50111757 0.49760002 0.49915469\n",
      " 0.49436269 0.49875031 0.49979797 0.49798583 0.49729924 0.50334215\n",
      " 0.49893116 0.49875431 0.50534058 0.50307745 0.50105133 0.50044965\n",
      " 0.50144261 0.49542712 0.50195443 0.50676543 0.50279273 0.50471365\n",
      " 0.50439284 0.4945317  0.49555938 0.51423164 0.50033903 0.49815622\n",
      " 0.50403826 0.49827108 0.50605638 0.50005793 0.50036581 0.49896987\n",
      " 0.49568199 0.49799145 0.50035779 0.50106937 0.50170568 0.49848582\n",
      " 0.50122511 0.50017013 0.50013939 0.5029171  0.50119897 0.50282099\n",
      " 0.50154819 0.4985736  0.49383324 0.50589403 0.49958976 0.4947248\n",
      " 0.50124478 0.4953099  0.48986831 0.50134729 0.50149515 0.49725957\n",
      " 0.49915901 0.49878293 0.49108915 0.49865014 0.49692674 0.49195991\n",
      " 0.49706845 0.49733817 0.5015111  0.50246844 0.49959178 0.50580252\n",
      " 0.49966851 0.49578447 0.49615624 0.50673422 0.49606612 0.50406629\n",
      " 0.49774558 0.49767308 0.49823673 0.49634198 0.49614986 0.49930363\n",
      " 0.49519486 0.50592309 0.49944839 0.49392142 0.49743616 0.4939884\n",
      " 0.49727182 0.5000446  0.49626612 0.50261645 0.49946051 0.49588394\n",
      " 0.49541914 0.49723823 0.49953997 0.49719724 0.50530805 0.50197365\n",
      " 0.50122796 0.51286691 0.49861742 0.50086489 0.50123638 0.49836221\n",
      " 0.49979617 0.50175385 0.4989234  0.49253029 0.49932893 0.49920469\n",
      " 0.49579698 0.50605854 0.49748022 0.50306934 0.50198822 0.49857719\n",
      " 0.49400288 0.50294007 0.50176397 0.49790513 0.49689483 0.50540142\n",
      " 0.49655791 0.50011683 0.50216863 0.50249171 0.50311687 0.49187811\n",
      " 0.49380608 0.49366762 0.49800317 0.50211645 0.50025093 0.50146691\n",
      " 0.49840497 0.49907853 0.50943861 0.49556153 0.50523416 0.49831955\n",
      " 0.49789186 0.49987779 0.498615   0.50334233 0.4897233  0.50317305\n",
      " 0.50154004 0.49640175 0.49961459 0.49616034 0.49913769 0.50146548\n",
      " 0.50247212 0.50063088 0.49396958 0.49244798 0.49978576 0.49964892\n",
      " 0.50304613 0.50486401 0.49833157 0.50263586 0.50614768 0.49223638\n",
      " 0.50374564 0.49997312 0.50024715 0.51803085 0.49756832 0.4988531\n",
      " 0.497363   0.49542444 0.49512634 0.4981129  0.50101986 0.50009601\n",
      " 0.49987551 0.49499173 0.49678952 0.49772831 0.50142081 0.49969304\n",
      " 0.5017663  0.50100925 0.49850321 0.49589877 0.49450843 0.49481651\n",
      " 0.49898204 0.49848155 0.4942315  0.49764575 0.50206755 0.49603527\n",
      " 0.49710802 0.49749613 0.49698373 0.50246482 0.50397038 0.49908829\n",
      " 0.49893867 0.5068684  0.50349887 0.50107426 0.50652919 0.50324454\n",
      " 0.49435066 0.49651583 0.49685966 0.5070637  0.49894526 0.50176743\n",
      " 0.4974639  0.49550573 0.50247433 0.49528158 0.49824572 0.49933876\n",
      " 0.50177422 0.50015784 0.49916497 0.5088184  0.49500866 0.50187664\n",
      " 0.49884258 0.49901211 0.49634602 0.49638632 0.48883126 0.49689633\n",
      " 0.49762968 0.4917104  0.4980203  0.49836494 0.49857836 0.50328416\n",
      " 0.50347521 0.50378028 0.50082698 0.4914231  0.50298645 0.49915696\n",
      " 0.49582038 0.49450598 0.4982243  0.5002017  0.49591208 0.49738569\n",
      " 0.49825412 0.50130578 0.50206455 0.49943249 0.50156015 0.4961021\n",
      " 0.50179182 0.50303964 0.50058425 0.49405008 0.49619398 0.50179683\n",
      " 0.49726307 0.49557477 0.49913056 0.49769344 0.50034978 0.49918777\n",
      " 0.49949785 0.49578801 0.5023415  0.49847723 0.50338435 0.5051075\n",
      " 0.49934545 0.50398746 0.50283814 0.50392523 0.499312   0.49609775\n",
      " 0.49698273 0.50240954 0.49697765 0.49840239 0.49431418 0.49713771\n",
      " 0.50282008 0.49439428 0.5004786  0.50038278 0.50744095 0.49688132\n",
      " 0.4946706  0.49511892 0.49755647 0.49776912 0.49656579 0.49748159\n",
      " 0.50175882 0.49863203 0.50266256 0.49842459 0.5076331  0.49560397\n",
      " 0.50120347 0.50062563 0.49607565 0.50388098 0.50085303 0.50104159\n",
      " 0.50453591 0.50186552 0.50173821 0.50544059 0.50062841 0.50088651\n",
      " 0.49745089 0.49449073 0.49804367 0.50303721 0.50654522 0.50287247\n",
      " 0.49840823 0.50350568 0.49884161 0.50462048 0.5052691  0.49950197\n",
      " 0.5087209  0.50359954 0.50265559 0.49905309 0.50190811 0.50771173\n",
      " 0.49654207 0.49808821 0.49551204 0.49817858 0.49822069 0.49765872\n",
      " 0.50664398 0.49688774 0.50093616 0.49964696 0.50190258 0.49950425\n",
      " 0.49606472 0.49741671 0.50803382 0.50118903 0.50227757 0.50397718\n",
      " 0.50320281 0.50893186 0.49997664 0.49864577 0.50168116 0.50000077\n",
      " 0.5022715  0.49983215 0.49913215 0.50377417 0.50290279 0.5016706\n",
      " 0.49998446 0.50235662 0.5013608  0.49954292 0.50281434 0.49912318\n",
      " 0.4990044  0.49870271 0.49210776 0.50176984 0.50067787 0.50137566\n",
      " 0.50251366 0.49833794 0.49864302 0.50622221 0.50103829 0.50173519\n",
      " 0.49921267 0.49480756 0.49760988 0.49861606 0.50072964 0.5036602\n",
      " 0.49398012 0.51059518 0.50069575 0.50103506 0.49882671 0.49584938\n",
      " 0.49218816 0.50440082 0.49745655 0.50154338 0.50787167 0.49907725\n",
      " 0.50195235 0.51010982 0.50150021 0.50600594 0.50708549 0.49627802\n",
      " 0.49612522 0.5004994  0.50157264 0.50158364 0.49492999 0.51076852\n",
      " 0.50166346 0.50442084 0.50603118 0.50194281 0.50063784 0.49529359\n",
      " 0.50148789 0.50573908 0.4959963  0.50114026 0.4982299  0.49261025\n",
      " 0.49944924 0.50609692 0.49899529 0.50037561 0.49674049 0.49876383\n",
      " 0.50273931 0.50290474 0.49263459 0.49732778 0.49967029 0.50492679\n",
      " 0.51137957 0.49872435 0.49582517 0.50172485 0.50114435 0.50172128\n",
      " 0.49535693 0.50262876 0.49387744 0.50229943 0.49736027 0.49850648\n",
      " 0.49835184 0.49632778 0.49680736 0.49574596 0.50143004 0.49943057\n",
      " 0.50220425 0.49774371 0.4991163  0.49828712 0.49691258 0.49653134\n",
      " 0.49892243 0.49999688 0.49626295 0.49652356 0.50188566 0.49862005\n",
      " 0.49286373 0.49764316 0.49940445 0.50098989 0.49821249 0.49914668\n",
      " 0.51039641 0.5068497  0.50073493 0.50071042 0.49615628 0.49356398\n",
      " 0.49709754 0.50640564 0.5004284  0.49360839 0.50076273 0.50541099\n",
      " 0.49789505 0.4993936  0.50166395 0.48602995 0.50122725 0.50275018\n",
      " 0.50011614 0.49698276 0.5024774  0.49552802 0.50273598 0.50054242\n",
      " 0.49766561 0.50077556 0.49874653 0.50282025 0.50167937 0.50040022\n",
      " 0.49993799 0.49603791 0.49729144 0.50217516 0.49716887 0.49783885\n",
      " 0.49936191 0.49741024 0.50151492 0.49804422 0.49460682 0.50738393\n",
      " 0.50056604 0.50814186 0.4967151  0.50123287 0.4992111  0.50262599\n",
      " 0.50514098 0.50075201 0.49969281 0.50072919 0.50204393 0.51167441\n",
      " 0.50505379 0.50189186 0.49611073 0.49957447 0.50411151 0.49959291\n",
      " 0.505271   0.49676822 0.49710802 0.49575774 0.4952178  0.50199208\n",
      " 0.50037313 0.49818929 0.49850265 0.49537093 0.50531976 0.49824503\n",
      " 0.5076329  0.49925712 0.50178846 0.50056885 0.49868123 0.49759042\n",
      " 0.49366941 0.50494732 0.49914909 0.50058557 0.49540291 0.50003164\n",
      " 0.50703624 0.4939644  0.4992521  0.50307344 0.49540727 0.50398993\n",
      " 0.50153007 0.50286215 0.49237448 0.49508325 0.49358249 0.50025411\n",
      " 0.50286581 0.50066479 0.49504544 0.49611662 0.49738127 0.4970918\n",
      " 0.50127508 0.50197494 0.50275967 0.49956716 0.50130475 0.49967573\n",
      " 0.5010296  0.49192397 0.49921464 0.49848836 0.50247914 0.50414382\n",
      " 0.49435526 0.49328146 0.49468385 0.49815107 0.50504251 0.50028913\n",
      " 0.4976616  0.50763767 0.49881132 0.49943157 0.50061354 0.51275829\n",
      " 0.49909059 0.49586355 0.49904599 0.49983452 0.49760374 0.50614772\n",
      " 0.49409415 0.49845388 0.50016948 0.50124731 0.49972576 0.50222361\n",
      " 0.4955866  0.49830615 0.4984688  0.5031157  0.49724419 0.49330812\n",
      " 0.49986206 0.5012038  0.50132084 0.49804993 0.50503199 0.49692818\n",
      " 0.50834256 0.50329643 0.49573782 0.49683163 0.49086559 0.5079128\n",
      " 0.50535559 0.50893398 0.50213463 0.5028848  0.49336915 0.49833056\n",
      " 0.50124253 0.49847858 0.49771969 0.50687282 0.5024004  0.49760699\n",
      " 0.49911142 0.49504863 0.50318608 0.49123282 0.50208633 0.49808091\n",
      " 0.49835014 0.4982047  0.51236427 0.50409727 0.49483287 0.49332027\n",
      " 0.49550319 0.50241008 0.52072476 0.49405435 0.49922616 0.49825513\n",
      " 0.5076014  0.49780447 0.50444215 0.49752243 0.49752155 0.49964244\n",
      " 0.50074221 0.50469942 0.49674414 0.50389914 0.50515019 0.49421672\n",
      " 0.49826468 0.51433302 0.50147799 0.49650289 0.49358801 0.50062972\n",
      " 0.50201204 0.50838865 0.49943613 0.49966342 0.49998457 0.49900007\n",
      " 0.5019911  0.50636582 0.49948617 0.50469351 0.49572229 0.49888966\n",
      " 0.49746703 0.5080025  0.49620676 0.49807897 0.49457184 0.49537751\n",
      " 0.50423023 0.50181341 0.4968397  0.50194522 0.49452698 0.50311148\n",
      " 0.49743889 0.5153053  0.5028477  0.49854537 0.49574165 0.5038403\n",
      " 0.50365817 0.49781062 0.49884487 0.49761703 0.50071394 0.49579713\n",
      " 0.50111189 0.50419159 0.50174096 0.50105823 0.49548694 0.49943926\n",
      " 0.5007997  0.49949848 0.49652807 0.5028728 ]\n",
      "[-1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1.\n",
      " -1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      "  1.  1. -1.  1.  1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      "  1. -1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1.\n",
      " -1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
      "  1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.\n",
      "  1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      " -1. -1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1. -1.\n",
      "  1.  1. -1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1.\n",
      "  1. -1.  1.  1.  1. -1. -1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1.\n",
      "  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1.  1. -1.\n",
      " -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.  1.\n",
      " -1. -1.  1. -1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1. -1.\n",
      " -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1.\n",
      " -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1. -1. -1.\n",
      " -1.  1. -1. -1. -1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1. -1. -1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1. -1.  1.  1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1.  1. -1.  1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1.  1.  1.\n",
      " -1.  1.  1.  1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      " -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1.  1. -1. -1.\n",
      " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1.\n",
      " -1.  1. -1.  1.  1.  1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1.  1. -1.\n",
      "  1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1. -1. -1.\n",
      " -1.  1. -1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.  1.  1.\n",
      " -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1.\n",
      "  1.  1. -1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1.\n",
      " -1. -1.  1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      "  1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1.\n",
      "  1.  1.  1.  1. -1. -1.  1. -1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracy\n",
    "lambda_ = 0.5\n",
    "beta = grad_descent[-1]\n",
    "prob = np.exp(np.dot(X_train, beta)) / (np.exp(np.dot(X_train, beta)) + 1)\n",
    "prediction = np.zeros(len(prob))\n",
    "\n",
    "for i in range(len(prob)):\n",
    "    if prob[i] > 0.5:\n",
    "        prediction[i] = 1\n",
    "    else:\n",
    "        prediction[i] = -1\n",
    "\n",
    "print(prob)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predictions</th>\n",
       "      <th>-1.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Truth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>285</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predictions  -1.0   1.0\n",
       "Truth                  \n",
       "-1            285   201\n",
       " 1            257   257"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_performance = pd.crosstab(y_train, prediction, rownames = ['Truth'], colnames = ['Predictions'])\n",
    "train_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = train_performance.iloc[0, 0]\n",
    "tp = train_performance.iloc[1, 1]\n",
    "fn = train_performance.iloc[1, 0]\n",
    "fp = train_performance.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.542\n",
      "Misclassification:  0.458\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fn + fp))\n",
    "print(\"Misclassification: \", (fp + fn) / (tp + tn + fn + fp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
